{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4a8cd3d-9835-40ee-8f40-0c1c559a84e0",
   "metadata": {},
   "source": [
    "# Building Transformer Models for Chatbots\n",
    "\n",
    "In this lesson we will take what we learned in the previous two weeks and apply them to building a simple chat bot.\n",
    "Our own GPT!\n",
    "\n",
    "## Pre-trained Embeddings\n",
    "\n",
    "## Next-word prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11f1e33a-9963-4b70-bda2-e873570d0147",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import re\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Layer, Embedding, Dense, LayerNormalization, Dropout, MultiHeadAttention\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3bb6a92-ed7f-4e1e-8890-bbd35e7f2da9",
   "metadata": {},
   "source": [
    "First things first, we need some text to use as training data.\n",
    "For this lessons, we'll use the complete works of Shakespeare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2f07f1d-c818-41de-9b83-b0033150d41d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** START OF THE PROJECT GUTENBERG EBOOK 100 ***\n",
      "The Complete Works of William Shakespeare\n",
      "\n",
      "by William Shakespeare\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                    Contents\n",
      "\n",
      "    THE SONNETS\n",
      "    ALL’S WELL THAT ENDS WELL\n",
      "    THE TRAGEDY OF ANTONY AND CLEOPATRA\n",
      "    AS YOU LIKE IT\n",
      "    THE COMEDY OF ERRORS\n",
      "    THE TRAGEDY OF CORIOLANUS\n",
      "    CYMBELINE\n",
      "    THE TRAGEDY OF HAMLET, PRINCE OF DENMARK\n",
      "    THE FIRST PART OF KING HENRY THE FOURTH\n",
      "    THE SECOND PART OF KING HENRY THE FOURTH\n",
      "    THE LIFE OF KING HENRY THE FIFTH\n",
      "    THE FIRST PART OF HENRY THE SIXTH\n",
      "    THE SECOND PART OF KING HENRY THE SIXTH\n",
      "    THE THIRD PART OF KING HENRY THE SIXTH\n",
      "    KING HENRY THE EIGHTH\n",
      "    THE LIFE AND DEATH OF KING JOHN\n",
      "    THE TRAGEDY OF JULIUS CAESAR\n",
      "    THE TRAGEDY OF KING LEAR\n",
      "    LOVE’S LABOUR’S LOST\n",
      "    THE TRAGEDY OF MACBETH\n",
      "    MEASURE FOR MEASURE\n",
      "    THE MERCHANT OF VENICE\n",
      "    THE MERRY WIVES OF WINDSOR\n",
      "    A MIDSUMMER NIGHT’S DREAM\n",
      "    MUCH ADO ABOUT NOTHING\n",
      "    THE TRAGEDY OF OTHELLO, THE MOOR OF VENICE\n",
      "    PERICLES, PRINC\n"
     ]
    }
   ],
   "source": [
    "# Load the text file\n",
    "path_to_file = tf.keras.utils.get_file(\"shakespeare.txt\", \n",
    "                                       \"https://www.gutenberg.org/files/100/100-0.txt\")\n",
    "\n",
    "with open(path_to_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Preview the first few lines\n",
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aba0e7b-da1b-403b-b3b9-5f07b6ed6ae1",
   "metadata": {},
   "source": [
    "We will now do some preprocessing to prepare our model for training.\n",
    "\n",
    "### Step 1. Preprocess the Text\n",
    "\n",
    "* Convert to lowercase (to reduce redundancy)\n",
    "* Remove special characters, but keep punctuation.\n",
    "* Tokenize into words (unlike last week where we used characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69111889-bb4c-4404-91e1-fd588fec236e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['start of the project gutenberg ebook 100', 'the complete works of william shakespeare', 'by william shakespeare', 'contents', 'the sonnets', 'alls well that ends well', 'the tragedy of antony and cleopatra', 'as you like it', 'the comedy of errors', 'the tragedy of coriolanus']\n"
     ]
    }
   ],
   "source": [
    "# Remove unnecessary characters, keeping punctuation and words\n",
    "text = re.sub(r\"[^a-zA-Z0-9.,;?!'\\\" \\n]\", \"\", text.lower())\n",
    "\n",
    "# Split into sentences (optional for training efficiency)\n",
    "sentences = text.split(\"\\n\")\n",
    "\n",
    "# Remove empty lines\n",
    "sentences = [s.strip() for s in sentences if len(s) > 0]\n",
    "\n",
    "# Print sample\n",
    "print(sentences[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370d025f-73b6-4fed-999f-e5c80f1a4506",
   "metadata": {},
   "source": [
    "### Use Keras Tokenizer\n",
    "\n",
    "Turn words into integer tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c7588b3-fba9-42bc-8a36-d26634eddbd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 56203\n",
      "Sample tokenized sentence: [2910, 5, 1, 6634, 20332, 20333, 13473]\n"
     ]
    }
   ],
   "source": [
    "# Initialize and fit tokenizer\n",
    "tokenizer = Tokenizer(filters=\"\")\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "\n",
    "# Convert text to sequences\n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "\n",
    "# Vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1  # +1 for padding token\n",
    "\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "print(f\"Sample tokenized sentence: {sequences[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4570ddbb-0720-4da1-9953-4d178ff0d887",
   "metadata": {},
   "source": [
    "### Create input sequences and output tokens\n",
    "\n",
    "We need to create the actual data the model will train on.\n",
    "* Input is a sequence of words\n",
    "* Output is the next word\n",
    "\n",
    "**E.G**\n",
    "> ```\n",
    "> [\"to\", \"be\", \"or\", \"not\"] -> \"to\"\n",
    "> ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "440efa47-898a-4e9f-86fb-a4e26df139e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample input: [   0    0    0    0    0    0 2910    5    1 6634]\n",
      "Target word index: 20332\n"
     ]
    }
   ],
   "source": [
    "# Create input-output sequences\n",
    "input_sequences = []\n",
    "output_words = []\n",
    "\n",
    "seq_length = 10  # Number of words per training sample\n",
    "\n",
    "for seq in sequences:\n",
    "    for i in range(1, len(seq)):\n",
    "        context = seq[max(0, i - seq_length):i]  # Previous words as input\n",
    "        target = seq[i]  # Next word as label\n",
    "        input_sequences.append(context)\n",
    "        output_words.append(target)\n",
    "\n",
    "# Pad sequences to have a uniform length\n",
    "# (Padding the front)\n",
    "input_sequences = pad_sequences(input_sequences, maxlen=seq_length, padding=\"pre\")\n",
    "\n",
    "# Convert output to numpy array\n",
    "output_words = np.array(output_words)\n",
    "\n",
    "print(f\"Sample input: {input_sequences[3]}\")\n",
    "print(f\"Target word index: {output_words[3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9897c0c9-ec83-4ada-aba6-555dd32a687d",
   "metadata": {},
   "source": [
    "### Step 5: Convert outputs to categorical labels\n",
    "\n",
    "We need to also convert the target vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d2f59e8-9f92-403f-8017-d4d168267cf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of input data: (809773, 10)\n",
      "Shape of output data: (809773, 1)\n"
     ]
    }
   ],
   "source": [
    "# Convert target words to one-hot encoding\n",
    "output_words = np.array(output_words, dtype=np.int32)  # Ensure integer type\n",
    "output_words = output_words.reshape(-1, 1)  # Reshape to match expected shape\n",
    "\n",
    "\n",
    "print(f\"Shape of input data: {input_sequences.shape}\")\n",
    "print(f\"Shape of output data: {output_words.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63fda26d-9352-442d-bf89-56f9948f5588",
   "metadata": {},
   "source": [
    "### Step 6: Prepare data for training\n",
    "\n",
    "Now that we have prepared our data for training, we can use tensorflow's datasets to prepare our data for efficient training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0656c4f1-0e4f-486f-819f-c644f793e2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the dataset for batch processing\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input_sequences, output_words))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True).prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384f1c5b-aa04-40fe-a19e-7d06549f1384",
   "metadata": {},
   "source": [
    "## Building the Transformer Model\n",
    "\n",
    "We now need to construct our MiniGPT model.\n",
    "We will create two classes `TransformerDecodeBlock` and `MiniGPT`.\n",
    "\n",
    "* `TransformerDecodeBlock` consists of a multi-head attention layer, as well as normalization layers and dense layers\n",
    "* `MiniGPT` adds the positional encoding, combines several decoders in sequence, and generates the  model's final output.\n",
    "\n",
    "Instead of sinusoidal encoding, our model initializes trainable positional embeddings as a tensor of zeros:\n",
    "\n",
    "```python\n",
    "    self.pos_embedding = tf.Variable(\n",
    "        initial_value=tf.zeros(shape=(1, max_len, embed_dim)), trainable=True, name=\"pos_embedding\"\n",
    "    )\n",
    "```\n",
    "\n",
    "At first, this means every position is represented identically. However, during training, the model learns to adjust these embeddings to best encode positional information based on the dataset.\n",
    "On models like GPT, this type of trainable positional encoding tends to perform better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f22b369d-3186-4b35-aa49-e81631439d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Layer, Embedding, Dense, LayerNormalization, Dropout, MultiHeadAttention\n",
    "\n",
    "\n",
    "# Define the transformer decoder block class\n",
    "\n",
    "class TransformerDecoderBlock(Layer):\n",
    "    def __init__(self, embed_dim: int, num_heads: int, ff_dim: int, dropout_rate: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.norm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.norm2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            Dense(ff_dim, activation=\"relu\"),  # Feedforward layer\n",
    "            Dense(embed_dim)  # Project back to embedding size\n",
    "        ])\n",
    "        self.dropout1 = Dropout(dropout_rate)\n",
    "        self.dropout2 = Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, inputs, training: bool = False):\n",
    "        seq_length = tf.shape(inputs)[1]  # Get sequence length dynamically\n",
    "        batch_size = tf.shape(inputs)[0]  # Get batch size dynamically\n",
    "\n",
    "        # Create a causal mask using TensorFlow operations\n",
    "        mask = tf.linalg.band_part(tf.ones((seq_length, seq_length)), -1, 0)  # Lower triangular mask\n",
    "        mask = tf.reshape(mask, (1, 1, seq_length, seq_length))  # Expand dims for broadcasting\n",
    "\n",
    "        # Apply masked self-attention\n",
    "        attention_output = self.attention(inputs, inputs, attention_mask=mask)\n",
    "        attention_output = self.dropout1(attention_output, training=training)\n",
    "        out1 = self.norm1(inputs + attention_output)  # Residual connection\n",
    "\n",
    "        # Feedforward network\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.norm2(out1 + ffn_output)  # Another residual connection\n",
    "\n",
    "\n",
    "# Define the MiniGPT class\n",
    "\n",
    "\n",
    "class MiniGPT(tf.keras.Model):\n",
    "    def __init__(self, vocab_size: int, embed_dim: int = 128, num_heads: int = 4, ff_dim: int = 512, num_layers: int = 3, max_len: int = 10):\n",
    "        super().__init__()\n",
    "        self.embedding = Embedding(vocab_size, embed_dim)\n",
    "        \n",
    "        # Use tf.Variable for positional embeddings instead of add_weight\n",
    "        self.pos_embedding = tf.Variable(\n",
    "            initial_value=tf.zeros(shape=(1, max_len, embed_dim)), trainable=True, name=\"pos_embedding\"\n",
    "        )\n",
    "\n",
    "        self.decoder_blocks = [TransformerDecoderBlock(embed_dim, num_heads, ff_dim) for _ in range(num_layers)]\n",
    "        self.final_layer = Dense(vocab_size)  # Output layer for token predictions\n",
    "\n",
    "    def call(self, inputs, training: bool = False):\n",
    "        x = self.embedding(inputs) + self.pos_embedding[:, :tf.shape(inputs)[1], :]\n",
    "        for block in self.decoder_blocks:\n",
    "            x = block(x, training=training)\n",
    "        return self.final_layer(x[:, -1, :])  # Predict only the last token\n",
    "\n",
    "\n",
    "\n",
    "# Instantiate the model w/ default values\n",
    "embed_dim = 128\n",
    "num_heads = 3\n",
    "ff_dim = 512\n",
    "num_layers = 4\n",
    "max_len = seq_length  # 10\n",
    "\n",
    "gpt_model = MiniGPT(vocab_size, embed_dim, num_heads, ff_dim, num_layers, max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36561df-75b2-4c72-9852-481b83ce4853",
   "metadata": {},
   "source": [
    "### Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ad6c6ea-c942-45a1-bfee-70f273a320ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=0.001, decay_steps=10000, decay_rate=0.9\n",
    ")\n",
    "\n",
    "gpt_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da62b6d-4aa2-4d1c-a5a8-f2f50b88ec00",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "We're ready to train the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3af1fdea-63f6-4ea4-a5ac-2118eb5005f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1739909192.746647    6826 service.cc:146] XLA service 0x7ff228037060 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1739909192.746697    6826 service.cc:154]   StreamExecutor device (0): Quadro RTX 5000, Compute Capability 7.5\n",
      "I0000 00:00:1739909192.746703    6826 service.cc:154]   StreamExecutor device (1): Quadro RTX 5000, Compute Capability 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m   21/12652\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:46\u001b[0m 8ms/step - accuracy: 0.0126 - loss: 10.6521      "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1739909201.934160    6826 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12652/12652\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 8ms/step - accuracy: 0.0314 - loss: 7.7398\n",
      "Epoch 2/10\n",
      "\u001b[1m12652/12652\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m99s\u001b[0m 8ms/step - accuracy: 0.0323 - loss: 7.3998\n",
      "Epoch 3/10\n",
      "\u001b[1m12652/12652\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m99s\u001b[0m 8ms/step - accuracy: 0.0323 - loss: 7.3600\n",
      "Epoch 4/10\n",
      "\u001b[1m12652/12652\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m99s\u001b[0m 8ms/step - accuracy: 0.0324 - loss: 7.3481\n",
      "Epoch 5/10\n",
      "\u001b[1m12652/12652\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m99s\u001b[0m 8ms/step - accuracy: 0.0325 - loss: 7.3442\n",
      "Epoch 6/10\n",
      "\u001b[1m12652/12652\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m99s\u001b[0m 8ms/step - accuracy: 0.0325 - loss: 7.3371\n",
      "Epoch 7/10\n",
      "\u001b[1m12652/12652\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m99s\u001b[0m 8ms/step - accuracy: 0.0327 - loss: 7.3358\n",
      "Epoch 8/10\n",
      "\u001b[1m12652/12652\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m99s\u001b[0m 8ms/step - accuracy: 0.0325 - loss: 7.3277\n",
      "Epoch 9/10\n",
      "\u001b[1m12652/12652\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m99s\u001b[0m 8ms/step - accuracy: 0.0325 - loss: 7.3220\n",
      "Epoch 10/10\n",
      "\u001b[1m12652/12652\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 8ms/step - accuracy: 0.0327 - loss: 7.3176\n",
      "CPU times: user 18min 33s, sys: 25.5 s, total: 18min 58s\n",
      "Wall time: 16min 46s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7ff3dc9f3690>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# This should take ~15 min on a powerful system.\n",
    "\n",
    "EPOCHS = 10\n",
    "\n",
    "gpt_model.fit(dataset, epochs=EPOCHS, batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b4afb8d-4182-4477-ae44-6be47e38cecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model and tokenizer for future use\n",
    "\n",
    "gpt_model.save(\"shakespeare_gpt.keras\")\n",
    "\n",
    "with open(\"tokenizer.pkl\", \"wb\") as f:\n",
    "    pickle.dump(tokenizer, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf73aeae-a59d-4f57-861c-a6c4b925aa80",
   "metadata": {},
   "source": [
    "In the future, we can use this code to load our saved model.\n",
    "\n",
    "\n",
    "```python\n",
    "\n",
    "# Load the trained model\n",
    "loaded_gpt_model = tf.keras.models.load_model(\"shakespeare_gpt.keras\")\n",
    "\n",
    "# Load tokenizer\n",
    "with open(\"tokenizer.pkl\", \"rb\") as f:\n",
    "    loaded_tokenizer = pickle.load(f)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0c01ac-6fac-4494-bc1f-b9d1ee2542d6",
   "metadata": {},
   "source": [
    "# Chatbot\n",
    "\n",
    "We now have a model which can take some text and predict the next word.\n",
    "Let's use that to create our chat bot.\n",
    "\n",
    "Our model will take in some seed text input by the user.\n",
    "It will then predict the next word, appending that to the seed text.\n",
    "This process repeats until the model is finished predicting text.\n",
    "\n",
    "We will try two methods to ge the next word:\n",
    "\n",
    "\n",
    "* **Greedy:** Select the model's most cofident prediction.\n",
    "* **Top-$k$ with temperature**: Look at the top predictions and select from them with weighted probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e927a4d6-b995-479e-a864-aaf73a814235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to be or not to the the the the the the the the the the the the the the the the the the the the\n"
     ]
    }
   ],
   "source": [
    "def generate_text_greedy(model, tokenizer, seed_text, max_length=20):\n",
    "    \"\"\"\n",
    "    Generate text using greedy decoding.\n",
    "    \n",
    "    :param model: Trained MiniGPT model\n",
    "    :param tokenizer: Keras tokenizer used for training\n",
    "    :param seed_text: Initial text prompt\n",
    "    :param max_length: Maximum words to generate\n",
    "    :return: Generated text\n",
    "    \"\"\"\n",
    "    sequence = tokenizer.texts_to_sequences([seed_text])[0]  # Convert seed to tokens\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        padded_sequence = tf.keras.preprocessing.sequence.pad_sequences([sequence], maxlen=10, padding=\"pre\")\n",
    "\n",
    "        # Get model prediction (logits over vocabulary)\n",
    "        predictions = model.predict(padded_sequence, verbose=0)\n",
    "        next_word_id = np.argmax(predictions)  # Greedy: pick most probable word\n",
    "\n",
    "        if next_word_id == 0:  # Stop if unknown token predicted\n",
    "            break\n",
    "\n",
    "        sequence.append(next_word_id)  # Add predicted word to sequence\n",
    "\n",
    "    return tokenizer.sequences_to_texts([sequence])[0]  # Convert tokens back to text\n",
    "\n",
    "\n",
    "prompt = \"To be or not to\"\n",
    "print(generate_text_greedy(gpt_model, tokenizer, prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2d2681-daa6-4af2-86d3-281d5fe70c36",
   "metadata": {},
   "source": [
    "### Temperature Scaling\n",
    "\n",
    "When the model predicts the next word, it assigns a probability to each word in the vocabulary. The temperature scales these probabilities before choosing the final word.\n",
    "\n",
    "Mathematically, temperature modifies the probability distribution as follows:\n",
    "\n",
    "$$P(w_i) = \\frac{\\exp\\left(\\frac{\\log P(w_i)}{T}\\right)}{\\sum_{j} \\exp\\left(\\frac{\\log P(w_j)}{T}\\right)}$$\n",
    "\n",
    "where:\n",
    "- $( P(w_i) )$ is the probability of word $( w_i )$\n",
    "- $( T )$ is the temperature parameter.\n",
    "- $( \\log P(w_i) )$ represents the original logits output by the model.\n",
    "- The denominator ensures the probabilities sum to 1.\n",
    "- A higher T makes the probabilities more uniform (increasing randomness).\n",
    "- A lower T makes the highest probability words dominate (more deterministic).\n",
    "\n",
    "By modifying $k$ and the temperature parameter, we can create a model which is more or less random in its responses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "59d5231a-6dde-44e2-8e56-7f7dc1c2bfea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to be or not to in the to the the of and and to of the the and to in of to and the and\n"
     ]
    }
   ],
   "source": [
    "def generate_text_top_k(model, tokenizer, seed_text, max_length=20, k=5, temperature=1.0):\n",
    "    \"\"\"\n",
    "    Generate text using top-k sampling.\n",
    "\n",
    "    :param model: Trained MiniGPT model\n",
    "    :param tokenizer: Keras tokenizer used for training\n",
    "    :param seed_text: Initial text prompt\n",
    "    :param max_length: Maximum words to generate\n",
    "    :param k: Number of top-k words to consider\n",
    "    :param temperature: Controls randomness (higher = more random)\n",
    "    :return: Generated text\n",
    "    \"\"\"\n",
    "    sequence = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        padded_sequence = tf.keras.preprocessing.sequence.pad_sequences([sequence], maxlen=10, padding=\"pre\")\n",
    "\n",
    "        # Get model prediction\n",
    "        predictions = model.predict(padded_sequence, verbose=0)\n",
    "        predictions = predictions.flatten()  # Convert shape (vocab_size,) for processing\n",
    "\n",
    "        # Apply temperature scaling\n",
    "        predictions = np.exp(predictions / temperature)\n",
    "        predictions = predictions / np.sum(predictions)  # Normalize to probability distribution\n",
    "\n",
    "        # Get top-k predictions\n",
    "        top_k_indices = np.argsort(predictions)[-k:]  # Get k highest probability words\n",
    "        top_k_probs = predictions[top_k_indices]\n",
    "\n",
    "        # Sample next word from top-k probabilities\n",
    "        next_word_id = np.random.choice(top_k_indices, p=top_k_probs / np.sum(top_k_probs))\n",
    "\n",
    "        if next_word_id == 0:\n",
    "            break  # Stop if unknown token predicted\n",
    "\n",
    "        sequence.append(next_word_id)\n",
    "\n",
    "    return tokenizer.sequences_to_texts([sequence])[0]\n",
    "\n",
    "prompt = \"To be or not to\"\n",
    "print(generate_text_top_k(gpt_model, tokenizer, prompt, k=5, temperature=0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "810792b8-e315-4a08-ae53-794de8a21f76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shakespeare GPT Chatbot - Type 'exit' to quit.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "You:  Hello\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: in and and and of of the in of the in the in of and and to and the of\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "You:  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Goodbye!\n"
     ]
    }
   ],
   "source": [
    "def chatbot():\n",
    "    print(\"Shakespeare GPT Chatbot - Type 'exit' to quit.\")\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"\\nYou: \")\n",
    "        if user_input.lower() == \"exit\":\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "\n",
    "        response = generate_text_top_k(gpt_model, tokenizer, user_input, k=5, temperature=2.8)\n",
    "        print(f\"Bot: {response}\")\n",
    "\n",
    "\n",
    "# Start Chatbot\n",
    "chatbot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2334edc5-63ad-4617-8c0b-3e9e9eb30076",
   "metadata": {},
   "source": [
    "## Closing Comments\n",
    "\n",
    "This model is pretty cool, but suffers from some obvious issues.\n",
    "\n",
    "* The model is very repititive\n",
    "* The model does not use punctuation\n",
    "* The output is not very natural sounding\n",
    "\n",
    "There are some advanced techniques we could implement to address them.\n",
    "* We could introduce a penalty to the output to repeated words (as was done in GPT 2).\n",
    "* We could manually split out input sentences where punctuation naturally occurs\n",
    "* We could try a bigger model, and train for longer\n",
    "* We could include more training text\n",
    "* We could increase the training context window (we only used 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e53ebff-ce0a-4082-905f-08d55f8937af",
   "metadata": {},
   "source": [
    "# Homework\n",
    "\n",
    "1. Download the novels below:\n",
    "\n",
    "* https://www.gutenberg.org/cache/epub/345/pg345.txt - [**Dracula**]\n",
    "* https://www.gutenberg.org/cache/epub/84/pg84.txt - [**Frankenstein**]\n",
    "* https://www.gutenberg.org/cache/epub/514/pg514.txt - [**Little Women**]\n",
    "* https://www.gutenberg.org/cache/epub/42671/pg42671.txt - [**Pride and Prejudice**]\n",
    "* https://www.gutenberg.org/cache/epub/64317/pg64317.txt - [**The Great Gatsby**]\n",
    "* https://www.gutenberg.org/cache/epub/2701/pg2701.txt - [**Moby Dick**]\n",
    "\n",
    "2. Perform text preprocessing on these texts.\n",
    "\n",
    "3. Build a tokenizer using the cleaned text.\n",
    "\n",
    "4. Train a GPT model using the cleaned text.\n",
    "\n",
    "5. Build your own chat bot using the trained GPT model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
